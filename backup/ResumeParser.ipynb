{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a412d1b-7237-47e3-9b7c-490b0ab986a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] =\"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9f8a0bc-00a0-4074-85e2-e75e036659c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# === Timestamp Formatter ===\n",
    "def timestamp():\n",
    "    return datetime.now().strftime(\"[%Y-%m-%d %H:%M:%S]\")\n",
    "\n",
    "# === Log Folder and File Path ===\n",
    "log_folder = Path(\"logs\")\n",
    "log_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "timestamp_str = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "log_path = log_folder / f\"log_{timestamp_str}.txt\"\n",
    "\n",
    "# === Start of the log ===\n",
    "with open(log_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"{timestamp()} BE Project — Resume Parsing Started\\n\")\n",
    "\n",
    "# === Log Writer Function (status-based) ===\n",
    "def write_log(filename, status, jd_match=None, error=None, details=None, final=False):\n",
    "    with open(log_path, \"a\", encoding=\"utf-8\") as f:\n",
    "        if status == \"START\":\n",
    "            f.write(f\"{timestamp()} Parsing Started: {filename}\\n\")\n",
    "        elif status == \"DETAILS\":\n",
    "            f.write(f\"{timestamp()} Resume Summary: {details}\\n\")\n",
    "        elif status == \"END\":\n",
    "            f.write(f\"{timestamp()} Parsing Completed: {filename}\\n\")\n",
    "        elif status == \"FAILED\":\n",
    "            f.write(f\"{timestamp()} Failed to Parse: {filename} | Error: {error}\\n\")\n",
    "        elif status == \"FINAL\" or final:\n",
    "            f.write(f\"{timestamp()} All Resumes Processed — Parsing Ended\\n\")\n",
    "\n",
    "def write_log(*args, **kwargs):\n",
    "    timestamp_str = datetime.now().strftime(\"[%Y-%m-%d %H:%M:%S]\")\n",
    "    with open(log_path, \"a\", encoding=\"utf-8\") as log_file:\n",
    "        # If just a plain message is passed\n",
    "        if len(args) == 1 and not kwargs:\n",
    "            log_file.write(f\"{timestamp_str} {args[0]}\\n\")\n",
    "        else:\n",
    "            filename = args[0] if len(args) > 0 else \"Unknown File\"\n",
    "            status = args[1] if len(args) > 1 else \"LOG\"\n",
    "            jd_match = kwargs.get(\"jd_match\")\n",
    "            error = kwargs.get(\"error\")\n",
    "            details = kwargs.get(\"details\")\n",
    "            final = kwargs.get(\"final\", False)\n",
    "\n",
    "            if status == \"START\":\n",
    "                log_file.write(f\"{timestamp_str} Parsing Started: {filename}\\n\")\n",
    "            elif status == \"DETAILS\":\n",
    "                log_file.write(f\"{timestamp_str} Resume Summary: {details}\\n\")\n",
    "            elif status == \"END\":\n",
    "                log_file.write(f\"{timestamp_str} Parsing Completed: {filename}\\n\")\n",
    "            elif status == \"FAILED\":\n",
    "                log_file.write(f\"{timestamp_str} Failed to Parse: {filename} | Error: {error}\\n\")\n",
    "            elif status == \"FINAL\" or final:\n",
    "                log_file.write(f\"{timestamp_str} All Resumes Processed — Parsing Ended\\n\")\n",
    "            else:\n",
    "                log_file.write(f\"{timestamp_str} {status}: {filename}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48070c71-80ec-404a-8d86-d4f20a450217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded JD from: jd.txt\n",
      "Processing: Priyal_Kalal_resume.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n",
      "C:\\Users\\adity\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The method `BaseChatModel.predict` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fallback] Using filename-based name: Priyal Kalal\n",
      "Moved Priyal_Kalal_resume.pdf to OutputResume_Folder\n",
      "\n",
      "Resume data saved to: C:\\Users\\adity\\BE Project\\BEProjectResumeParser\\Resume_Parsed_CSVs\\resume_summary.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "import re\n",
    "\n",
    "# === Configuration ===\n",
    "jd_folder = Path(\"JD_Folder\")\n",
    "input_folder = Path(\"InputResume_Folder\")\n",
    "output_folder = Path(\"OutputResume_Folder\")\n",
    "csv_output_folder = Path(\"Resume_Parsed_CSVs\")\n",
    "csv_path = csv_output_folder / \"resume_summary.csv\"\n",
    "\n",
    "output_folder.mkdir(parents=True, exist_ok=True)\n",
    "csv_output_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def write_log(msg):\n",
    "    now = pd.Timestamp.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    with open(\"log.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"[{now}] {msg}\\n\")\n",
    "\n",
    "def extract_name_from_filename(filename):\n",
    "    base = Path(filename).stem.lower()\n",
    "    parts = base.replace(\"resume\", \"\").replace(\"cv\", \"\").replace(\"-\", \" \").replace(\"_\", \" \").split()\n",
    "    clean_name = \" \".join(word.capitalize() for word in parts if word not in [\"resume\", \"cv\", \"final\", \"updated\"])\n",
    "    return clean_name\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    reader = PdfReader(str(pdf_path))\n",
    "    for page in reader.pages:\n",
    "        if page.extract_text():\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "def chunk_text(text, max_tokens=3000):\n",
    "    sentences = text.split('. ')\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    for sentence in sentences:\n",
    "        if len(current_chunk) + len(sentence) < max_tokens:\n",
    "            current_chunk += sentence + \". \"\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence + \". \"\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    return chunks\n",
    "\n",
    "def summarize_pdf(pdf_path, jd):\n",
    "    resume_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "    # Extract email, phone, LinkedIn\n",
    "    def extract_contact_details(text):\n",
    "        if not isinstance(text, str):\n",
    "            return '', '', ''\n",
    "        \n",
    "        email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
    "        email_matches = re.findall(email_pattern, text)\n",
    "        email = email_matches[0] if email_matches else ''\n",
    "        \n",
    "        phone_pattern = r'(?:\\+91[\\-\\s]?|91[\\-\\s]?|0)?(?:[6-9]\\d{2}[\\s-]?\\d{3}[\\s-]?\\d{4})'\n",
    "        phone_matches = re.findall(phone_pattern, text)\n",
    "        phone = phone_matches[0].replace(\" \", \"\").replace(\"-\", \"\") if phone_matches else ''\n",
    "\n",
    "        linkedin_pattern = r'(https?://)?(www\\.)?linkedin\\.com/(in|pub)/[a-zA-Z0-9\\-_/]+'\n",
    "        linkedin_match = re.search(linkedin_pattern, text)\n",
    "        linkedin = linkedin_match.group(0) if linkedin_match else ''\n",
    "\n",
    "\n",
    "        return email, phone, linkedin\n",
    "\n",
    "    email, phone, linkedin = extract_contact_details(resume_text)\n",
    "    chunks = chunk_text(resume_text)\n",
    "    summaries = []\n",
    "\n",
    "    llm = ChatOpenAI(temperature=0.7, max_tokens=1000, model=\"gpt-3.5-turbo\")\n",
    "    for chunk in chunks:\n",
    "        prompt_template = PromptTemplate(\n",
    "            input_variables=[\"text\", \"jd\"],\n",
    "            template=\"\"\"\n",
    "You are an AI assistant. Summarize the relevant resume details below in relation to the following job description.\n",
    "\n",
    "Resume Chunk:\n",
    "\\\"\\\"\\\"{text}\\\"\\\"\\\"\n",
    "\n",
    "Job Description:\n",
    "\\\"\\\"\\\"{jd}\\\"\\\"\\\"\n",
    "\n",
    "Extract the key points in concise bullet form, JSON-style if possible. Do NOT return full JSON yet.\n",
    "\"\"\"\n",
    "        )\n",
    "        prompt = prompt_template.format(text=chunk, jd=jd)\n",
    "        partial_summary = llm.predict(prompt)\n",
    "        summaries.append(partial_summary)\n",
    "\n",
    "    merge_prompt = f\"\"\"\n",
    "You are an expert resume evaluator.\n",
    "\n",
    "Below are summaries of different chunks of a resume, all based on the same candidate:\n",
    "\n",
    "\\\"\\\"\\\"{''.join(summaries)}\\\"\\\"\\\"\n",
    "\n",
    "Job Description:\n",
    "\\\"\\\"\\\"{jd}\\\"\\\"\\\"\n",
    "\n",
    "Now based on the summaries and the JD, extract the following structured JSON. \n",
    "\n",
    "Return clean JSON output with this format:\n",
    "{{\n",
    "    \"Name\": \"<Extracted full name of the candidate>\",\n",
    "    \"JD Match\": \"<% Match>\",\n",
    "    \"Missing Keywords\": {{\n",
    "        \"Technical Skills\": [],\n",
    "        \"Tools & Technologies\": [],\n",
    "        \"Concepts & Methodologies\": []\n",
    "    }},\n",
    "    \"Top Matching Keywords\": [],\n",
    "    \"Profile Summary\": \"<Brief summary related to JD>\",\n",
    "    \"Projects\": [\n",
    "        {{\n",
    "            \"Project Name\": \"<Title>\",\n",
    "            \"Relevance to JD\": \"<High/Medium/Low>\",\n",
    "            \"Technologies Used\": [],\n",
    "            \"Impact\": \"<Project outcomes>\"\n",
    "        }}\n",
    "    ],\n",
    "    \"Certifications & Courses\": [\"<Relevant Certifications>\"],\n",
    "    \"Skills That Will Contribute to the Company\": [],\n",
    "    \"Soft Skills & Leadership Qualities\": [\"<Communication, Leadership, etc.>\"],\n",
    "    \"Industry Experience\": \"<Relevant industries like Finance, Healthcare>\",\n",
    "    \"Culture Fit Assessment\": \"<High/Medium/Low – Explanation>\",\n",
    "    \"Potential Concerns\": [\"<Gaps, missing skills, weaknesses>\"],\n",
    "    \"Red Flags & Risk Analysis\": [\"<Major issues>\"],\n",
    "    \"Candidate’s Growth Potential\": \"<How much they can grow in the company>\",\n",
    "    \"Effort Needed by the Company\": \"<Low/Medium/High – Explanation>\",\n",
    "    \"Resume Strength Score\": \"<Numeric score between 0.0 and 10.0>\",\n",
    "    \"Relevant Experience (yrs)\": \"<Years of directly relevant experience>\",\n",
    "    \"Employment Gaps Detected\": true,\n",
    "    \"Resume Format Quality\": \"<Good/Average/Poor>\",\n",
    "    \"Candidate Type\": \"<Junior/Mid-Level/Senior>\",\n",
    "    \"HR Notes\": \"<Any special observations for HR>\"\n",
    "}}\n",
    "\n",
    "Return only the JSON.\n",
    "\"\"\"\n",
    "    llm_merge = ChatOpenAI(temperature=0.3, max_tokens=1500, model=\"gpt-3.5-turbo\")\n",
    "    final_response = llm_merge.predict(merge_prompt)\n",
    "\n",
    "    try:\n",
    "        structured_data = json.loads(final_response)\n",
    "        gpt_name = structured_data.get(\"Name\", \"\").strip()\n",
    "        fallback_name = extract_name_from_filename(pdf_path.name)\n",
    "\n",
    "        if not gpt_name or \"candidate\" in gpt_name.lower() or gpt_name.startswith(\"<\"):\n",
    "            print(f\"[Fallback] Using filename-based name: {fallback_name}\")\n",
    "            structured_data[\"Name\"] = fallback_name\n",
    "\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Failed to parse JSON for {pdf_path.name}\")\n",
    "        structured_data = {\"Name\": extract_name_from_filename(pdf_path.name)}\n",
    "\n",
    "    structured_data[\"Email\"] = email\n",
    "    structured_data[\"Phone\"] = phone\n",
    "    structured_data[\"LinkedIn\"] = linkedin\n",
    "\n",
    "    return structured_data\n",
    "\n",
    "# === Load JD ===\n",
    "jd_files = list(jd_folder.glob(\"*.txt\"))\n",
    "if not jd_files:\n",
    "    print(\"No JD file found. Please add a .txt JD in JD_Folder.\")\n",
    "    exit()\n",
    "\n",
    "latest_jd_file = max(jd_files, key=lambda f: f.stat().st_mtime)\n",
    "with open(latest_jd_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    jd = f.read()\n",
    "print(f\"Loaded JD from: {latest_jd_file.name}\")\n",
    "\n",
    "# === Parse Resumes ===\n",
    "pdf_paths = list(input_folder.glob(\"*.pdf\"))\n",
    "if not pdf_paths:\n",
    "    print(\"No resumes found in the input folder.\")\n",
    "    exit()\n",
    "\n",
    "all_data = []\n",
    "write_log(\"BE Project Resume Parsing Start\")\n",
    "\n",
    "for pdf_path in pdf_paths:\n",
    "    filename = pdf_path.name\n",
    "    print(f\"Processing: {filename}\")\n",
    "    write_log(f\"Parsing started: {filename}\")\n",
    "\n",
    "    try:\n",
    "        parsed_data = summarize_pdf(pdf_path, jd)\n",
    "        parsed_data[\"resume_name\"] = filename\n",
    "        all_data.append(parsed_data)\n",
    "        write_log(f\"Details: {json.dumps(parsed_data, indent=2)}\")\n",
    "        write_log(f\"Completed: {filename}\")\n",
    "    except Exception as e:\n",
    "        write_log(f\"Failed: {filename} | Error: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "    shutil.move(str(pdf_path), str(output_folder / filename))\n",
    "    print(f\"Moved {filename} to OutputResume_Folder\\n\")\n",
    "\n",
    "# === Save Final CSV ===\n",
    "df = pd.DataFrame(all_data)\n",
    "\n",
    "# Move contact fields before resume_name\n",
    "cols = df.columns.tolist()\n",
    "for field in ['LinkedIn', 'Phone', 'Email']:\n",
    "    if field in cols and 'resume_name' in cols:\n",
    "        f_idx = cols.index(field)\n",
    "        r_idx = cols.index('resume_name')\n",
    "        cols.insert(r_idx, cols.pop(f_idx))\n",
    "df = df[cols]\n",
    "\n",
    "try:\n",
    "    if csv_path.exists() and os.path.getsize(csv_path) > 0:\n",
    "        existing_df = pd.read_csv(csv_path)\n",
    "        combined_df = pd.concat([existing_df, df], ignore_index=True)\n",
    "    else:\n",
    "        combined_df = df\n",
    "except pd.errors.EmptyDataError:\n",
    "    print(\"Existing CSV is empty or corrupt. Starting fresh.\")\n",
    "    combined_df = df\n",
    "\n",
    "combined_df.to_csv(csv_path, index=False)\n",
    "print(f\"Resume data saved to: {csv_path.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b78125-85e8-4ca0-b0c2-059420946b41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b770cb-d731-492d-a840-bce64138945d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f1705b-7a66-4e08-bd40-84d338031aaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
