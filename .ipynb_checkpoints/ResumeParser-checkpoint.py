{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a412d1b-7237-47e3-9b7c-490b0ab986a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] =\"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9f8a0bc-00a0-4074-85e2-e75e036659c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# === Timestamp Formatter ===\n",
    "def timestamp():\n",
    "    return datetime.now().strftime(\"[%Y-%m-%d %H:%M:%S]\")\n",
    "\n",
    "# === Log Folder and File Path ===\n",
    "log_folder = Path(\"logs\")\n",
    "log_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "timestamp_str = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "log_path = log_folder / f\"log_{timestamp_str}.txt\"\n",
    "\n",
    "# === Start of the log ===\n",
    "with open(log_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"{timestamp()} BE Project â€” Resume Parsing Started\\n\")\n",
    "\n",
    "# === Log Writer Function (status-based) ===\n",
    "def write_log(filename, status, jd_match=None, error=None, details=None, final=False):\n",
    "    with open(log_path, \"a\", encoding=\"utf-8\") as f:\n",
    "        if status == \"START\":\n",
    "            f.write(f\"{timestamp()} Parsing Started: {filename}\\n\")\n",
    "        elif status == \"DETAILS\":\n",
    "            f.write(f\"{timestamp()} Resume Summary: {details}\\n\")\n",
    "        elif status == \"END\":\n",
    "            f.write(f\"{timestamp()} Parsing Completed: {filename}\\n\")\n",
    "        elif status == \"FAILED\":\n",
    "            f.write(f\"{timestamp()} Failed to Parse: {filename} | Error: {error}\\n\")\n",
    "        elif status == \"FINAL\" or final:\n",
    "            f.write(f\"{timestamp()} All Resumes Processed â€” Parsing Ended\\n\")\n",
    "\n",
    "def write_log(*args, **kwargs):\n",
    "    timestamp_str = datetime.now().strftime(\"[%Y-%m-%d %H:%M:%S]\")\n",
    "    with open(log_path, \"a\", encoding=\"utf-8\") as log_file:\n",
    "        # If just a plain message is passed\n",
    "        if len(args) == 1 and not kwargs:\n",
    "            log_file.write(f\"{timestamp_str} {args[0]}\\n\")\n",
    "        else:\n",
    "            filename = args[0] if len(args) > 0 else \"Unknown File\"\n",
    "            status = args[1] if len(args) > 1 else \"LOG\"\n",
    "            jd_match = kwargs.get(\"jd_match\")\n",
    "            error = kwargs.get(\"error\")\n",
    "            details = kwargs.get(\"details\")\n",
    "            final = kwargs.get(\"final\", False)\n",
    "\n",
    "            if status == \"START\":\n",
    "                log_file.write(f\"{timestamp_str} Parsing Started: {filename}\\n\")\n",
    "            elif status == \"DETAILS\":\n",
    "                log_file.write(f\"{timestamp_str} Resume Summary: {details}\\n\")\n",
    "            elif status == \"END\":\n",
    "                log_file.write(f\"{timestamp_str} Parsing Completed: {filename}\\n\")\n",
    "            elif status == \"FAILED\":\n",
    "                log_file.write(f\"{timestamp_str} Failed to Parse: {filename} | Error: {error}\\n\")\n",
    "            elif status == \"FINAL\" or final:\n",
    "                log_file.write(f\"{timestamp_str} All Resumes Processed â€” Parsing Ended\\n\")\n",
    "            else:\n",
    "                log_file.write(f\"{timestamp_str} {status}: {filename}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48070c71-80ec-404a-8d86-d4f20a450217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Loaded JD from: jd.txt\n",
      "Processing: resume_6.pdf\n",
      "Moved resume_6.pdf to OutputResume_Folder\n",
      "\n",
      "Processing: resume_7.pdf\n",
      "Moved resume_7.pdf to OutputResume_Folder\n",
      "\n",
      "Processing: resume_8.pdf\n",
      "Moved resume_8.pdf to OutputResume_Folder\n",
      "\n",
      "Processing: resume_9.pdf\n",
      "Moved resume_9.pdf to OutputResume_Folder\n",
      "\n",
      " Data appended and saved to: C:\\Users\\adity\\BE Project\\Stage4\\Resume_Parsed_CSVs\\resume_summary.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# === Configuration ===\n",
    "jd_folder = Path(\"JD_Folder\")\n",
    "input_folder = Path(\"InputResume_Folder\")\n",
    "output_folder = Path(\"OutputResume_Folder\")\n",
    "csv_output_folder = Path(\"Resume_Parsed_CSVs\")\n",
    "csv_path = csv_output_folder / \"resume_summary.csv\"\n",
    "\n",
    "output_folder.mkdir(parents=True, exist_ok=True)\n",
    "csv_output_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# === Logging ===\n",
    "def write_log(msg):\n",
    "    now = pd.Timestamp.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    with open(\"log.txt\", \"a\") as f:\n",
    "        f.write(f\"[{now}] {msg}\\n\")\n",
    "\n",
    "# === PDF Text Extraction ===\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    reader = PdfReader(str(pdf_path))\n",
    "    for page in reader.pages:\n",
    "        if page.extract_text():\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "# === Text Chunking ===\n",
    "def chunk_text(text, max_tokens=3000):\n",
    "    sentences = text.split('. ')\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    for sentence in sentences:\n",
    "        if len(current_chunk) + len(sentence) < max_tokens:\n",
    "            current_chunk += sentence + \". \"\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence + \". \"\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    return chunks\n",
    "\n",
    "# === Summarize Resume ===\n",
    "def summarize_pdf(pdf_path, jd):\n",
    "    resume_text = extract_text_from_pdf(pdf_path)\n",
    "    chunks = chunk_text(resume_text)\n",
    "    summaries = []\n",
    "\n",
    "    # Summarize each chunk\n",
    "    llm = ChatOpenAI(temperature=0.7, max_tokens=1000, model=\"gpt-3.5-turbo\")\n",
    "    for chunk in chunks:\n",
    "        prompt_template = PromptTemplate(\n",
    "            input_variables=[\"text\", \"jd\"],\n",
    "            template=\"\"\"\n",
    "You are an AI assistant. Summarize the relevant resume details below in relation to the following job description.\n",
    "\n",
    "Resume Chunk:\n",
    "\\\"\\\"\\\"\n",
    "{text}\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "Job Description:\n",
    "\\\"\\\"\\\"\n",
    "{jd}\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "Extract the key points in concise bullet form, JSON-style if possible. Do NOT return full JSON yet.\n",
    "\"\"\"\n",
    "        )\n",
    "        prompt = prompt_template.format(text=chunk, jd=jd)\n",
    "        partial_summary = llm.predict(prompt)\n",
    "        summaries.append(partial_summary)\n",
    "\n",
    "    # Final merge prompt\n",
    "    merge_prompt = f\"\"\"\n",
    "You are an expert resume evaluator.\n",
    "\n",
    "Below are summaries of different chunks of a resume, all based on the same candidate:\n",
    "\n",
    "\\\"\\\"\\\"\n",
    "{''.join(summaries)}\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "Job Description:\n",
    "\\\"\\\"\\\"\n",
    "{jd}\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "Return clean JSON output with this format:\n",
    "{{\n",
    "    \"Name\": \"<Candidate Name>\",\n",
    "    \"JD Match\": \"<% Match>\",\n",
    "    \"Missing Keywords\": {{\n",
    "        \"Technical Skills\": [],\n",
    "        \"Tools & Technologies\": [],\n",
    "        \"Concepts & Methodologies\": []\n",
    "    }},\n",
    "    \"Top Matching Keywords\": [],\n",
    "    \"Profile Summary\": \"<Brief summary related to JD>\",\n",
    "    \"Projects\": [\n",
    "        {{\n",
    "            \"Project Name\": \"<Title>\",\n",
    "            \"Relevance to JD\": \"<High/Medium/Low>\",\n",
    "            \"Technologies Used\": [],\n",
    "            \"Impact\": \"<Project outcomes>\"\n",
    "        }}\n",
    "    ],\n",
    "    \"Certifications & Courses\": [\"<Relevant Certifications>\"],\n",
    "    \"Skills That Will Contribute to the Company\": [],\n",
    "    \"Soft Skills & Leadership Qualities\": [\"<Communication, Leadership, etc.>\"],\n",
    "    \"Industry Experience\": \"<Relevant industries like Finance, Healthcare>\",\n",
    "    \"Culture Fit Assessment\": \"<High/Medium/Low â€“ Explanation>\",\n",
    "    \"Potential Concerns\": [\"<Gaps, missing skills, weaknesses>\"],\n",
    "    \"Red Flags & Risk Analysis\": [\"<Major issues>\"],\n",
    "    \"Candidateâ€™s Growth Potential\": \"<How much they can grow in the company>\",\n",
    "    \"Effort Needed by the Company\": \"<Low/Medium/High â€“ Explanation>\",\n",
    "    \"Resume Strength Score\": \"<Numeric score between 0.0 and 10.0>\",\n",
    "    \"Relevant Experience (yrs)\": \"<Years of directly relevant experience>\",\n",
    "    \"Employment Gaps Detected\": true,\n",
    "    \"Relevant Projects Count\": 0,\n",
    "    \"Resume Format Quality\": \"<Good/Average/Poor>\",\n",
    "    \"Candidate Type\": \"<Junior/Mid-Level/Senior>\",\n",
    "    \"HR Notes\": \"<Any special observations for HR>\"\n",
    "}}\n",
    "Return only the JSON.\n",
    "\"\"\"\n",
    "    llm_merge = ChatOpenAI(temperature=0.3, max_tokens=1500, model=\"gpt-3.5-turbo\")\n",
    "    final_response = llm_merge.predict(merge_prompt)\n",
    "\n",
    "    try:\n",
    "        structured_data = json.loads(final_response)\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\" Failed to parse JSON for {pdf_path.name}\")\n",
    "        structured_data = {}\n",
    "\n",
    "    return structured_data\n",
    "\n",
    "# === Load Latest JD ===\n",
    "jd_files = list(jd_folder.glob(\"*.txt\"))\n",
    "if not jd_files:\n",
    "    print(\" No JD file found. Please add a .txt JD in JD_Folder.\")\n",
    "    exit()\n",
    "\n",
    "latest_jd_file = max(jd_files, key=lambda f: f.stat().st_mtime)\n",
    "with open(latest_jd_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    jd = f.read()\n",
    "print(f\"ðŸ“„ Loaded JD from: {latest_jd_file.name}\")\n",
    "\n",
    "# === Main Processing Loop ===\n",
    "pdf_paths = list(input_folder.glob(\"*.pdf\"))\n",
    "if not pdf_paths:\n",
    "    print(\" No resumes found in the input folder.\")\n",
    "    exit()\n",
    "\n",
    "all_data = []\n",
    "write_log(\" BE Project Resume Parsing Start\")\n",
    "\n",
    "for pdf_path in pdf_paths:\n",
    "    filename = pdf_path.name\n",
    "    print(f\"Processing: {filename}\")\n",
    "    write_log(f\" Parsing started: {filename}\")\n",
    "\n",
    "    try:\n",
    "        parsed_data = summarize_pdf(pdf_path, jd)\n",
    "        parsed_data[\"resume_name\"] = filename\n",
    "        all_data.append(parsed_data)\n",
    "        write_log(f\"Details: {json.dumps(parsed_data, indent=2)}\")\n",
    "        write_log(f\"Completed: {filename}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        write_log(f\" Failed: {filename} | Error: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "    shutil.move(str(pdf_path), str(output_folder / filename))\n",
    "    print(f\"Moved {filename} to OutputResume_Folder\\n\")\n",
    "\n",
    "# === Save to CSV (Append Mode) ===\n",
    "df = pd.DataFrame(all_data)\n",
    "\n",
    "try:\n",
    "    if csv_path.exists() and os.path.getsize(csv_path) > 0:\n",
    "        existing_df = pd.read_csv(csv_path)\n",
    "        combined_df = pd.concat([existing_df, df], ignore_index=True)\n",
    "    else:\n",
    "        combined_df = df\n",
    "except pd.errors.EmptyDataError:\n",
    "    print(\" Existing CSV is empty or corrupt. Starting fresh.\")\n",
    "    combined_df = df\n",
    "\n",
    "combined_df.to_csv(csv_path, index=False)\n",
    "print(f\" Data appended and saved to: {csv_path.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b78125-85e8-4ca0-b0c2-059420946b41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b770cb-d731-492d-a840-bce64138945d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f1705b-7a66-4e08-bd40-84d338031aaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
